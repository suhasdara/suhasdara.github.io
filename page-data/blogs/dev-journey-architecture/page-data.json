{
    "componentChunkName": "component---src-templates-blog-temp-js",
    "path": "/blogs/dev-journey-architecture",
    "result": {"data":{"mdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"The Development Journey for Choosing an Architecture\",\n  \"date\": \"2021-10-14T00:00:00.000Z\",\n  \"slug\": \"/blogs/dev-journey-architecture\",\n  \"tags\": [\"tech\"],\n  \"postType\": \"blog\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"This blog describes the journey that I, a developer took to decide the\\narchitecture to use for the implementation of a recent project I have been\\nworking on. It demonstrates the thought process a developer goes through to\\ncome to a resolution.\"), mdx(\"p\", null, \"Let's commence the journey!\"), mdx(\"p\", null, \"When you have an idea that you want to implement, as an individual developer,\\nor maybe even as a part of an organization, you look for the cheapest and\\neasiest solutions for the architecture of your implementation. That is exactly\\nwhat I did as well when I started the implementation of my web project (\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"top\\nsecret\"), \"). My project involves dealing with a lot of data, but originally, I\\nthought that it wasn't nearly enough data to justify the need for a backend. I\\ndecided to stick with my familiarity and host a static website for free on\\nGitHub Pages as I have previously done (this website!).\"), mdx(\"h3\", null, \"Static website with GitHub Pages\"), mdx(\"h4\", null, \"Problem 1\"), mdx(\"p\", null, \"As I started generating and cleaning my data, I realized that in a static\\nwebsite, any data that I store would be directly accessible by anyone visiting\\nthe website by simply opening the developer console in the browser and looking\\nat the network traffic or the sources. This was a no-go in my situation as I\\nneeded the data to not be visible directly to the user. I researched numerous\\nways of encryption and decryption and decided to use AES to encrypt all the\\ndata and only decrypt something when it is needed. I thought that would be it,\\nbut that's when I ran into another problem.\"), mdx(\"h4\", null, \"Problem 2\"), mdx(\"p\", null, \"As I generated more data, I realized I had upwards of 600MB of data. Now, for\\na static website, this is a problem, because every time the website is\\naccessed, all that data will have to be loaded by the browser. Slow internet\\nconnections will really struggle with the website. However, I was\\ntunnel-visioned to use a free static website host and was determined to make\\nit work. But with more time and a few days of wasted work, I decided to\\nabandon the static website approach and concluded that I would need a proper\\nbackend or at least a database.\"), mdx(\"h3\", null, \"The quest for a database\"), mdx(\"p\", null, \"The data that I had was a lot of nested JSONs (up to 5 or 6 levels deep). This\\ndata was hierarchical in nature and a structured database such as MySQL or\\nPostgreSQL would be of no use, or at the very least, it would be a lot of work\\nto transform the data into a format that would suit those structured\\ndatabases. Alright, so let's look at the options.\"), mdx(\"h4\", null, \"Heroku\"), mdx(\"p\", null, \"I first looked at Heroku's free tier. Unfortunately, while Heroku did have\\nfree options to run a backend service, I wasn't satisfied with their support\\nfor NoSQL databases. They offered \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Redis\"), \" and \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MongoDB\"), \". While Redis was\\nsuitable for my project, the free limit was far too low, and MongoDB was\\noutright unsuitable based on my prior experience.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"500px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"12.6%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Heroku Redis\",\n    \"title\": \"Heroku Redis\",\n    \"src\": \"/static/0f07098a524ca43c06ec21da6474c4b1/0b533/heroku.png\",\n    \"srcSet\": [\"/static/0f07098a524ca43c06ec21da6474c4b1/0b533/heroku.png 500w\", \"/static/0f07098a524ca43c06ec21da6474c4b1/487bb/heroku.png 698w\"],\n    \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"h4\", null, \"Amazon Web Services (AWS)\"), mdx(\"p\", null, \"I then thought I would take a look at AWS. AWS offered \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DynamoDB\"), \", a NoSQL\\ndatabase with monthly free limits that were well suited for my purposes.\\nHowever, I did not have a very good time with AWS in the past when I used it\\nfor two university projects (\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"%5Cprojects%5Cdeepfakes\"\n  }, \"Deepfakes\"), \" and\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"%5Cprojects%5Ctravel-scares-me\"\n  }, \"Travel Scares Me\"), \") because the documentation is\\nvery unnerving, and you often have to rely on tutorials. I kept this as a\\nbackup plan if I could not find anything better.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"500px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"16.400000000000002%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"AWS DynamoDB\",\n    \"title\": \"AWS DynamoDB\",\n    \"src\": \"/static/73a77a34e5f8f9d60b94d12c3282de79/0b533/aws.png\",\n    \"srcSet\": [\"/static/73a77a34e5f8f9d60b94d12c3282de79/0b533/aws.png 500w\", \"/static/73a77a34e5f8f9d60b94d12c3282de79/2ba99/aws.png 1221w\"],\n    \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"h4\", null, \"Microsoft Azure\"), mdx(\"p\", null, \"Next up, Azure. I had never used or considered Azure before for any of my\\nprojects. This probably was a bias for me when I looked at Azure \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CosmosDB\"), \".\\nSimilar to Heroku, they offered support for \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MongoDB\"), \", but also for\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cassandra\"), \", which I had not used before and wasn't exactly sure was right\\nfor my use case. With my existing bias, when I noticed that CosmosDB was only\\nfree for 12 months, I steered away (sorry Azure).\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"500px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"15.6%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Azure CosmosDB\",\n    \"title\": \"Azure CosmosDB\",\n    \"src\": \"/static/badaed6ddcfdc2392b8925af7016f49a/0b533/azure.png\",\n    \"srcSet\": [\"/static/badaed6ddcfdc2392b8925af7016f49a/0b533/azure.png 500w\", \"/static/badaed6ddcfdc2392b8925af7016f49a/5b587/azure.png 1010w\"],\n    \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"h4\", null, \"Google Cloud Platform (GCP)\"), mdx(\"p\", null, \"Last stop was GCP. I had previously only used GCP for getting access to API\\nkeys for Google Maps and Google Translate. I had some familiarity with their\\nconsole. So, imagine my delight when I learned that the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cloud Firestore\"), \"\\nwas created to solve problems almost identical to mine. Remember when I said\\nmy data was all JSONs? Well, straight from Firestore documentation: \\\"You may\\nnotice that documents look a lot like JSON. In fact, they basically are. ...\\nin general, you can treat documents as lightweight JSON records.\\\"\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"500px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"12%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Cloud Firestore\",\n    \"title\": \"Cloud Firestore\",\n    \"src\": \"/static/8e02f73ab617ef43ca88430a999ea420/0b533/gcp.png\",\n    \"srcSet\": [\"/static/8e02f73ab617ef43ca88430a999ea420/0b533/gcp.png 500w\", \"/static/8e02f73ab617ef43ca88430a999ea420/764be/gcp.png 806w\"],\n    \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"p\", null, \"However, this time I wanted to be more careful and make sure all my\\nrequirements can be fulfilled by GCP before moving forward with Firestore. I\\nfound that Google Compute Engine (GCE) provided a free virtual machine (VM)\\ninstance that, from a preliminary estimate, could host both my frontend and\\nbackend. I was all set, or so I thought.\"), mdx(\"h3\", null, \"The data dilemma\"), mdx(\"h4\", null, \"Shock #1\"), mdx(\"p\", null, \"As I said before, I had a lot of data. However, this data was unchanging. This\\nmeant that I had to write the data to Firestore only once, and read as\\nnecessary in the future. As I thought about that, I was hit with an unpleasant\\nrealization. If I had to write 600MB of data to Firestore as individual JSON\\ndocuments, that would amount to over 5,500,000 writes. The daily limit for\\nwrites to Firestore is 20,000. It would take me 9 months to write all my data\\nto Firestore if I wanted to stick to free limits! At that one moment, I\\nwished I had chosen DynamoDB which has a monthly limit as opposed to a daily\\nlimit. But I decided to soldier on.\"), mdx(\"p\", null, \"I found ways to reduce the size of my data to 150MB and even shift some of the\\ndata computation to the backend instead of computing everything beforehand and\\npre-caching. This brought down the writes to about 6000! So, I immediately\\ndecided to test this - at worst I would lose a day or two messing with the\\ndata while staying under the free limits...\"), mdx(\"h4\", null, \"Shock #2\"), mdx(\"p\", null, \"Firestore reads have a limit too, 50,000 a day to be precise. By shifting a\\nmajority of the computation to the backend from pre-caching, I had essentially\\nskyrocketed the number of reads I would be needing to serve each request by\\nthe frontend. I was needing close to 1300 reads to serve a single request!\\nThis means I would only be able to serve 38 requests a day, which is simply\\npathetic. This essentially made the idea of using Firestore for pre-caching\\nthe data unviable.\"), mdx(\"h4\", null, \"Resolution\"), mdx(\"p\", null, \"After battling with different amounts of pre-caching in Firestore, something\\nsuddenly clicked in my mind. Why do I even need to pre-cache in Firestore!?\\nI could pre-cache in the backend itself in-memory. It would not only be\\nfaster to work with in-memory computations, but I won't have to deal with\\nFirestore as much anymore. It would result in a lengthy cold start for the\\nfirst request because of the minimal amount of reads from Firestore and some\\nrelatively heavy computation involved in the pre-caching, but this was\\nsomething I was ready to deal with. So now all that was left was to set up\\ninstances for the frontend and backend.\"), mdx(\"h3\", null, \"Hosting the website\"), mdx(\"p\", null, \"When I had picked GCP as the cloud platform to host my website, I was only\\naware of Google Compute Engine (GCE) as a platform to host services.\"), mdx(\"h4\", null, \"Google Compute Engine (GCE)\"), mdx(\"p\", null, \"GCE's free tier includes a single e2-micro instance, which provides 0.25 vCPUs\\n(virtual CPUs) and 1GB RAM. Even though my preliminary estimates had convinced\\nme that this would be sufficient to host both my frontend and backend, I had\\nstarted to second guess that estimate. Additionally, working with GCE is akin\\nto working on a fresh virtual machine because GCE is an \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Infrastructure_as_a_service\"\n  }, \"Infrastructure as a\\nService (IaaS)\"), \".\\nI was not keen on having to deal with disk images and especially network\\nrouting, so I started looking for alternatives within GCP.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"500px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"16.599999999999998%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Google Compute Engine\",\n    \"title\": \"Google Compute Engine\",\n    \"src\": \"/static/8d4d15fcd8ba232cb9416d28a16915d3/0b533/gcp-gce.png\",\n    \"srcSet\": [\"/static/8d4d15fcd8ba232cb9416d28a16915d3/0b533/gcp-gce.png 500w\", \"/static/8d4d15fcd8ba232cb9416d28a16915d3/e4a12/gcp-gce.png 963w\"],\n    \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"h4\", null, \"Google App Engine (GAE)\"), mdx(\"p\", null, \"As I scoured the GCP documentation and products, I found GAE. As opposed to\\nGCE, GAE is a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Platform_as_a_service\"\n  }, \"Platform as a Service\\n(PaaS)\"), \". GAE provides\\nF-instances and B-instances which each scale in a different manner -\\nF-instances autoscale, B-instances are manually scaled. GAE's free tier\\nincludes 28hrs/day of F1-instances (256MB RAM, 600MHz CPU limit) and 9hrs/day\\nof B2-instances (512MB RAM, 1.2GHz CPU limit). Historically, F-instances were\\nintended for frontend services, and B-instances were intended for backend\\nservices. Though that is not the case anymore, I decided to follow it that\\nway.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"500px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"21.2%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Google App Engine\",\n    \"title\": \"Google App Engine\",\n    \"src\": \"/static/9d3580c75f87a124775395f647c9c043/0b533/gcp-gae.png\",\n    \"srcSet\": [\"/static/9d3580c75f87a124775395f647c9c043/0b533/gcp-gae.png 500w\", \"/static/9d3580c75f87a124775395f647c9c043/a016c/gcp-gae.png 762w\"],\n    \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"p\", null, \"I deployed my frontend and backend services to GAE using an F1-instance and\\na B2-instance respectively. It was as simple as creating two YAML files for\\nthe configuration of the instances and another YAML file for the routing\\nbetween the instances. The frontend service worked perfectly without any\\nhassles, however, my backend service showed some erratic behavior by randomly\\nshutting down.\"), mdx(\"p\", null, \"I wasn't exactly sure what the problem was so the first thing I did was to run\\nmemory profilers on my backend application. I learned one important lesson by\\ndoing this. There is a difference between how much space the data takes on\\ndisk in JSON format as compared to using Python dictionaries in memory. While\\non disk the data was 150MB, the backend process was using closer to 380MB.\\nInterestingly, this was still within the B2-instance memory limit. I\\nstruggled to find an answer for the erratic behavior and even posted to\\nStackOverflow which I don't usually do unless I am desperate (which I was at\\nthis point).\"), mdx(\"p\", null, \"I started to look for other solutions to the problem, and that's when I found\\nyet another product of GCP... The gift that keeps giving, isn't it?\"), mdx(\"h4\", null, \"Google Cloud Run\"), mdx(\"p\", null, \"I randomly found Cloud Run when I was looking at the GCP dashboard. I didn't\\nknow what it did, but it was under the same category as GAE, so I tried\\nresearching it. I learned that Cloud Run is also a PaaS product that can be\\nused as a replacement for GAE and has even lesser maintenance overheads (no\\nneed for even a YAML configuration!) Additionally, the scaling of Cloud Run\\nis easier to understand than having to pick from the different instance\\noptions that GAE offers.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"500px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"18.999999999999996%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Google Cloud Run\",\n    \"title\": \"Google Cloud Run\",\n    \"src\": \"/static/3e637c51f7c66a867c894afe7bc2268f/0b533/gcp-run.png\",\n    \"srcSet\": [\"/static/3e637c51f7c66a867c894afe7bc2268f/0b533/gcp-run.png 500w\", \"/static/3e637c51f7c66a867c894afe7bc2268f/9d76a/gcp-run.png 829w\"],\n    \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"p\", null, \"I also learned that GAE uses virtual machines underneath (essentially built\\nover GCE) while Cloud Run is built over Google Kubernetes Engine (GKE).\\nConsidering how much Google is now pushing Kubernetes as a scaling solution,\\nI am surprised Cloud Run does not seem as popular as App Engine (judging\\nbased on the resources available outside documentation).\"), mdx(\"p\", null, \"Anyways, Cloud Run had a really simple learning curve, and I could essentially\\nmove my backend service that was built for GAE to Cloud Run with almost no\\nchanges and just Dockerizing my app. I was however concerned about the free\\ntier limits as they felt rather cryptic to the naked eye. However,\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://cloud.google.com/products/calculator\"\n  }, \"GCP's Pricing Calculator\"), \"\\nseemed to promise great limits within the free tier based on my needs. Note\\nthat I still believed that 512MB of RAM would be sufficient for my service\\nbased on my App Engine experience.\"), mdx(\"p\", null, \"I deployed my backend to Cloud Run and started testing it. To my\\ndisappointment, the service was still erratically shutting down. I dove into\\ndebugging once again. Based on a hunch and logs (yes, Cloud Run has logs!), I\\ninvestigated the memory usage once again. I read about the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://psutil.readthedocs.io/en/latest/#psutil.Process.memory_info\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"memory_info\"), \"\\nfunction\"), \"\\nin the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"psutil\"), \" module in Python that I was using for memory profiling and\\nlearned about two different metrics for the memory usage of a process! What I\\nhad originally looked at was the Resident Set Size (RSS) which amounted to\\nabout 380MB. However, what was important to consider was the Virtual Memory\\nSize (VMS) or the total amount of memory used by a process including shared\\nlibraries (Should have paid more attention in that OS class...)\\nWell, this VMS metric revealed that my service actually required closer to\\n850MB memory. I think this is a lesson to stay light on the number of\\nlibraries your application is using.\"), mdx(\"p\", null, \"Stumped with being unable to reduce my process size significantly, I decided\\nto simply increase the memory limit to 1GB in my Cloud Run instance. And just\\nlike that, my backend service started working as intended. However, I am still\\nmonitoring the billing aspect of this memory limit increase, which seems to\\nbe nil for the moment. In hindsight, I wonder if the shift from GAE to Cloud\\nRun was even necessary. It does seem to bring larger benefits, more\\nflexibility, and simpler to understand scaling of instances, so I can only\\nhope it wasn't in vain!\"), mdx(\"h3\", null, \"Epilogue\"), mdx(\"p\", null, \"As a developer, this blog describes a journey that probably was relatable to\\nyou if you are a developer as well. You often start with assumptions that may\\nor may not be true. You explore different paths to try and build your product\\nunder those assumptions, but they just won't work at times. But with the\\nendless amount of technology now available, and with the entire Cloud at our\\ndisposal, the developer journey sure has become more enjoyable.\"));\n}\n;\nMDXContent.isMDXComponent = true;","timeToRead":8,"fields":{"excerpt":"This blog describes the journey that I, a developer took to decide the\narchitecture to use for the implementation of a recent project I have been\nworking on. It demonstrates the thought process a developer goes through to\ncome to a resolution.","miscTimeToRead":1},"frontmatter":{"title":"The Development Journey for Choosing an Architecture","date":"2021-10-14T00:00:00.000Z","tags":["tech"],"slug":"/blogs/dev-journey-architecture"}}},"pageContext":{}},
    "staticQueryHashes": ["63159454"]}